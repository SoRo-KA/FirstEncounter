# Field Observation Report ‚Äì Incidental Human‚ÄìRobot Encounters  
**Observer:** Abdulrahman Alaql  
**Event / Location:** Kingdom Tower  
**Date:** 29 / 30 October  
**Observation time block:** 10:00 ‚Äì 17:00  

**Robot mode:** Autonomous  
**Crowd level:** Low  
**Noise level:** Medium  

---

## üß© Notable Incidents  

### 1. Elderly Foreign Visitor and Turn-Taking Breakdown  
An elderly foreign visitor tried to have a long, philosophical conversation with the robot. However, the robot kept responding too soon‚Äîafter one or two sentences‚Äîbefore the visitor finished speaking. The man struggled to keep the flow, trying to continue talking without stopping so the robot wouldn‚Äôt interrupt. He became visibly confused and frustrated when the robot repeatedly spoke over him.  

**Why notable:**  
This revealed a serious timing issue: the robot failed to detect when a person had finished speaking. The mismatch in turn-taking created awkward and tiring interactions for users, especially for slower or older speakers.  

---

### 2. Handshake vs. Wave Confusion  
Whenever the robot greeted people with ‚ÄúHello,‚Äù it raised both arms forward and waved. Many nearby visitors interpreted this gesture as an invitation to shake hands. Several attempted to extend their hand, expecting the robot to reciprocate. The robot, however, continued waving, leaving visitors momentarily confused or embarrassed.  

**Why notable:**  
A repeated social misunderstanding between ‚Äúwave‚Äù and ‚Äúhandshake‚Äù gestures caused awkwardness and shows how easily body-language cues can be misread in human‚Äìrobot encounters.  

---

### 3. Visual-Description Skill Not Triggering  
At least twice, visitors held up objects (like cups) and asked the robot, ‚ÄúWhat do you see?‚Äù expecting it to describe what was visible. The correct skill failed to trigger, and the robot replied, ‚ÄúI can‚Äôt know what you‚Äôre holding,‚Äù even though it had the visual ability to identify objects.  

**Why notable:**  
The visual-description skill appears to depend on very narrow trigger words. The mismatch between capability and activation phrases disappointed visitors and reduced trust in the robot‚Äôs intelligence.  

---

### 4. Philosophical Dialogue About *M3GAN*  
A fluent English-speaking visitor engaged the robot in a deep conversation about the horror movie *M3GAN*, asking whether robots could become dangerous or justify harmful behavior. She repeatedly tried to elicit a ‚Äúscary‚Äù or morally wrong response. The robot consistently emphasized human safety and its ethical boundaries.  

**Why notable:**  
This conversation stood out for its depth and social complexity. It showed the robot‚Äôs ability to handle provocative or philosophical topics calmly and responsibly, reinforcing public trust in its design ethics.  

---

## üë• Typical Behaviors Observed  

### 1. Turn-Taking Difficulties  
The robot often interrupted speakers before they finished, responding prematurely or missing mid-sentence corrections. Visitors struggled to adapt and found the flow unnatural.  

**Interpretation:**  
Turn-taking and speech-end detection are poorly tuned, leading to overlaps and miscommunication.  

---

### 2. Seeking Human Confirmation Before Interaction  
Most first-time visitors sought reassurance from nearby staff members, asking if they could talk to the robot or what it could do. Only after seeing others interact did they approach it directly.  

**Interpretation:**  
Visitors lacked clear cues that the robot was interactive. Human presence provided social validation.  

---

### 3. Watching Without Engaging  
Many visitors were visibly amazed, watching for 10‚Äì15 seconds, taking pictures or videos, but never initiating a conversation. Possible reasons include shyness, language barriers, or uncertainty about how to start.  

**Interpretation:**  
Environmental layout and unclear affordances discouraged spontaneous engagement.  

---

### 4. Limited Use of Action Commands  
Few people knew they could ask the robot to perform actions (wave, spin, handshake). Most interactions stayed conversational.  

**Interpretation:**  
Visitors perceived the robot as a talk-only system because its motion skills weren‚Äôt signaled clearly.  

---

### 5. Response Delays and Confusion  
Long or inconsistent delays between user speech and robot replies caused visitors to repeat themselves or assume the robot hadn‚Äôt heard them. Sometimes the robot then misheard and gave irrelevant answers.  

**Interpretation:**  
Response latency breaks conversational rhythm and reduces confidence.  

---

### 6. Variation in Engagement Levels  
Engagement ranged from deep curiosity to total indifference. Some visitors spent up to 10 minutes interacting, others only a few seconds.  

**Interpretation:**  
Interest varies by personality, familiarity with technology, and social comfort, not only technical background.  

---

### 7. Human-First Contact Habit  
Visitors frequently followed typical booth behavior patterns‚Äîapproaching staff instead of the robot. They feared looking awkward or breaking etiquette.  

**Interpretation:**  
Exhibition norms conditioned them to expect humans, not robots, to answer first.  

---

## üåç Environmental or Accessibility Observations  

With 2‚Äì3 staff members and onlookers crowding around, the robot‚Äôs interaction zone often felt intimidating. People hesitated to step forward or interrupt ongoing conversations.  

**Interpretation:**  
Booth design and staff proximity reduced perceived accessibility.  

---

## üí° Reflections / Suggestions  

1. **Low Success Rate:**  
   Only about one in twenty conversations felt smooth and natural. Delays, mishearing, and interruptions dominated.  
   **Suggestions:** Tune voice activity detection, improve turn-taking logic, and analyze failed interactions for retraining.  

2. **Delays and Feedback:**  
   Delays cause confusion because visitors cannot tell if the robot is ‚Äúthinking‚Äù or ‚Äúdidn‚Äôt hear.‚Äù  
   **Suggestions:** Show visual or sound indicators (lights, motion, ‚ÄúLet me think‚Ä¶‚Äù) and shorten average latency.  

3. **Greeting and Ending:**  
   Initially, greetings were lively but endings were abrupt. After adding a ‚Äúgoodbye‚Äù skill with waving and verbal closure, interactions felt much more natural.  
   **Suggestions:** Continue refining openings and closings with friendly transitional phrases.  

4. **Need for Guidance:**  
   Interaction success rose sharply when a facilitator demonstrated how to talk to the robot or modeled good microphone positioning and question phrasing.  
   **Suggestions:** Always have a guide nearby or display on-screen tips and prompts to reduce dependence on staff.  

5. **Accent and Volume Sensitivity:**  
   Native or fluent English speakers achieved smoother exchanges; accented or quiet speakers often failed.  

6. **Clarifying Interactivity:**  
   Many visitors defaulted to speaking to staff, so clear signals of interactivity are needed.  
   **Suggestions:** Add signage or auditory prompts (‚ÄúSay hi to me!‚Äù).  

---

## üìä Summary Counts  

| Category | Count |
|-----------|-------|
| Total people observed | 200 |
| Looked at robot | 150 |
| Stopped (no talk) | 100 |
| Talked briefly (1‚Äì2 turns) | 30 |
| Deep conversation (‚â•3 turns) | 10 |
| Avoided / negative reactions | 5 |

---
